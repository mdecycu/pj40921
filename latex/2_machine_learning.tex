\chapter{相關文獻探討}
\section{回顧 Solid Edge 和 Femap 在機械設計相關專題與研究}


 典型的類神經網路架構，如(圖.\ref{類神經網路架構})所示每一層的每一個神經元都會連接到下一層全部的神經元，對於每個神經元輸出會有不同的權重。\\

 神經元是AI系統中使用的數學模型，其行為與實際的大腦神經元運作方式相仿，模型以數字的方式表達，神經元間傳遞會有不同強度，而以數值的大小代表不同強度，這個數值我們稱為權重，對結果產生重要的影響。\\

 (圖.\ref{類神經網路架構})基礎的類神經網路架構主要由輸入層、隱藏層和輸出層這三部分組成，實際運用上還有更多樣更複雜的類神經網路架構，深度學習則是有更多的隱藏層，從意義上來說就是增加了類神經網路的深度。\\

 此外，如(圖.\ref{類神經網路架構})所示，資料由輸入層傳入，經過隱藏層運算和記憶，再由輸出層進行輸出，這種資料被傳遞的方式被稱為前饋輸入(feed-forward)。\\

 類神經網路架構有了記憶，就能進一步讓網路學習。當類神經網路接收資料並猜測答案，如果答案與實際答案不符、有落差或有錯誤的情況，它會回授並修改對每個神經元權重和偏差修正的程度，並嘗試調配各項數值來修正輸出的結果，讓結果的正確性提高，這樣的修正行為就被稱為反向傳播(back-propagation)。透過迭代方法進行反複試驗，模擬人們學習的行為，而每一次的迭代被稱為epoch，經過一定的迭帶次數後會透過反向傳播修正輸出的誤差，經過不斷執行的修正，最終類神經網路的學習會不斷進步並給出更好的答案，訓練時間長短取決於訓練項目的複雜程度。\\

 可以看到該神經網絡的輸出僅取決於互連的權重，還取決於神經元本身的偏差，雖然權重會影響啟動函數曲線的陡度，但是偏差會將發生變化的整個曲線，向右或向左，權重和偏差的選擇，決定了單個神經元的預測強度，而訓練類神經網絡使用的輸入數據可以來微調權重和偏差。(圖.\ref{類神經網路關係})\\

\newpage

\subsection{工程參照}
工程參照是Solid Edge的一種特有功能，直覺來說就是快速建模，只需要選擇需要的模型形狀，再將數值一一打入，即使不用畫草圖也能迅速的將模型產生。\\

在工作列上的工具裡能很輕易的找到工程參照，點開有高達13項的模型提供選擇。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=16cm]{螢幕擷取畫面 2023-05-11 200432}
\caption{\Large 開啟位置}\label{開啟位置}
\end{center}
\end{figure}
\\
開啟(圖.\ref{開啟畫面}):\\

每種模型的界面都大同小異，有各個尺寸的代號、參數調整，因為還能算出簡易的分析，所以也有支撐(固定)、負載與設定材質的功能，模型預覽的功能因只能做出較簡單的畫面變更，所以只有軸設計器與梁設計器有而已。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{軸設計器1}
\caption{\Large 開啟畫面}\label{開啟畫面}
\end{center}
\end{figure}
\\
材質(圖.\ref{材質畫面}、圖.\ref{內建材質}):\\

模型材質的更改，有提供常用的材料可以選擇，如果沒有需要的材料的話也能透過彈性模量(楊氏係數)、剛性模量及比值量來達成設定。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=8cm]{軸設計器6}
\caption{\Large 材質畫面}\label{材質畫面}
\includegraphics[width=8cm]{螢幕擷取畫面 2023-05-11 205001}
\caption{\Large 內建材質}\label{內建材質}
\end{center}
\end{figure}
\\

分析結果(圖.\ref{分析結果畫面}):\\

設定參數完成後，按下計算就能直接得到力的分析結果，各種力作用及平面都能自由調整來得到需要的數值。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{軸設計器7}
\caption{\Large 分析結果畫面}\label{分析結果畫面}
\end{center}
\end{figure}
\\

以下介紹工程參照的各種設計器：
\begin{itemize}
		%=----------Sigmoid      Function----------=%
	\item 軸設計器(圖.\ref{軸設計器})：\\
		在做參數設計只需要打上想要的尺寸即可得到想要的模型，上面預覽所呈現的藍線部分就是對應到下面軸參數的數值，也能隨時改變剖面的類型，自由度非常的高，但須注意的是預覽部分並不會將各個剖面的類型顯現出來，所以預覽的畫面並非是建立出來的完整模型\\

		支撐功能裡的支撐編號是預覽畫面中的藍色三角形，左邊的三角形是程式預設的固定邊，而右邊的三角形會根據單元編號的改變所移動，支撐編號只有1(預設固定邊)與2(單元編號)提供選擇\\

		負載功能也有一些相較簡單的力能輸入，力的位置也會顯示在預覽畫面上，材質部分也有預設一些比較常用到的材料，如果沒有的話，也只要將需要的材料性質輸入進去即可\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{軸設計器2}
		\caption{\Large 軸設計器}\label{軸設計器}
		\end{center}
		\end{figure}
		\\
		剖面選擇的數值是模型從左到右開始算的第幾個剖面(圖.\ref{剖面選擇})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{00}
		\caption{\Large 剖面選擇}\label{剖面選擇}
		\end{center}
		\end{figure}
		\\
		新增的功能就是在所選擇的剖面左/右邊新增新的剖面(圖.\ref{2.8})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{00}
		\caption{\Large 新增}\label{2.8}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.9})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{軸設計器7}
		\caption{\Large 記算結果}\label{2.9}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.10})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{軸模型}
		\caption{\Large 軸模型}\label{2.10}
		\end{center}
		\end{figure}
		\\
		
\newpage

	
	\item 凸輪設計器(圖.\ref{2.11}) :\\
		與軸設計器相同，一樣有著參數以及負載的數值可以輸入，上面畫面一樣是各數值所代表的部位，而在凸輪參數最下列還有一項周速的數值是不可調整的，是直接經由上列數值所計算出來的
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{凸輪1}
		\caption{\Large 凸輪設計器}\label{2.11}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.12})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{禿倫}
		\caption{\Large 凸輪材質}\label{2.12}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.13})\\
		
		點擊計算後會來到計算結果的頁面，左上畫面是左下角圖形選項的結果，有位移、速度、加速度和躍度提供選擇；而右上的圖形是設計參數計算出來的凸輪輪廓，藍色線是基圓紅色線則是凸輪的形狀；在來是段的選擇，總共有3段的選項，分別是1(0°~120°)、2(120°~240°)、3(240°~360°)
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{凸輪2}
		\caption{\Large 凸輪計算結果}\label{2.13}
		\end{center}
		\end{figure}
		\\
		
		各段的設定也都是可以各別做調整的，右邊也會有一些常用的最大數值及時計算出來提供參考(圖.\ref{2.14}、圖.\ref{2.15}、圖.\ref{2.16})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{凸輪4}
		\caption{\Large 凸輪0°-120°}\label{2.14}
		\end{center}
		\end{figure}
		\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{凸輪5}
		\caption{\Large 凸輪120°-240°}\label{2.15}
		\end{center}
		\end{figure}
		\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{凸輪6}
		\caption{\Large 凸輪240°-360°}\label{2.16}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.17})
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=1cm]{凸輪模型}
		\caption{\Large 凸輪模型}\label{2.17}
		\end{center}
		\end{figure}
		\\
		
\newpage
	
	\item 直齒輪設計器(圖.\ref{2.18}) :\\
		開啟畫面一樣是分成3大部分，代號部位介紹、齒輪參數和計算參數
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{直齒輪1}
		\caption{\Large 直齒輪設計器}\label{2.18}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.19})\\
		參數設計的輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{螢幕擷取畫面 2023-05-18 200646}
		\caption{\Large 細部選項}\label{2.19}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.20})\\
		計算結果這邊因設計器的預設是設計一組齒輪的，所以結果會呈現兩顆齒輪聶合的基本資訊，也會有兩顆齒輪在運動時所產生的各向力作用，以及強度驗證的通過與否\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{直齒輪5}
		\caption{\Large 直齒輪計算結果}\label{2.20}
		\end{center}
		\end{figure}
		\\
		係數(圖.\ref{2.21})\\
		計算後結果的各項係數\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{直齒輪6}
		\caption{\Large 直齒輪計算結果係數}\label{2.21}
		\end{center}
		\end{figure}
		\\
		齒輪參數(圖.\ref{2.22})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{直齒輪7}
		\caption{\Large 係數齒輪參數}\label{2.22}
		\end{center}
		\end{figure}
		\\
		計算出的幾何參數(圖.\ref{2.23})\\
		各項設定計算出來的幾何參數，也是齒輪最終各部位的基本尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{直齒輪8}
		\caption{\Large 直齒輪幾何參數}\label{2.23}
		\end{center}
		\end{figure}
		\\
		建立模型(圖.\ref{2.24})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{正齒輪模型}
		\caption{\Large 直齒輪模型}\label{2.24}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 錐齒輪設計器(圖.\ref{2.25}) :\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{錐齒輪1}
		\caption{\Large 錐齒輪設計器}\label{2.25}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.26}) \\
		能調整要計算的分析選項及結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{錐齒輪2}
		\caption{\Large 設計參數選項}\label{2.26}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.27}) \\
		材質的設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{錐齒輪3}
		\caption{\Large 錐齒輪材質}\label{2.27}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.28}) \\
		設計計算後的分析結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{錐齒輪4}
		\caption{\Large 錐齒輪計算結果}\label{2.28}
		\end{center}
		\end{figure}
		\\
		計算出的幾何參數(圖.\ref{2.29}) \\
		根據設計參數所生成之錐齒輪的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{錐齒輪5}
		\caption{\Large 錐齒輪幾何參數}\label{2.29}
		\end{center}
		\end{figure}
		\\
		建立模型(圖.\ref{2.228}) \\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{錐齒輪模型}
		\caption{\Large 錐齒輪模型}\label{2.228}
		\end{center}
		\end{figure}
		\\
	
\newpage	
	
	\item 渦輪設計器(圖.\ref{2.30}) :\\
		有預設的組立件尺寸供使用者直接選擇\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{渦輪1}
		\caption{\Large 渦輪設計器}\label{2.30}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.31}) \\
		更細部的各項設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{渦輪2}
		\caption{\Large 渦桿渦輪細部選項}\label{2.31}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.32}) \\
		渦桿與渦輪的材質設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{渦輪3}
		\caption{\Large 渦桿渦輪材質}\label{2.32}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.33}) \\
		設計計算後的分析結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{渦輪4}
		\caption{\Large 渦桿渦輪計算結果}\label{2.33}
		\end{center}
		\end{figure}
		\\
		係數(圖.\ref{2.34}) \\
		計算後結果的各項係數\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{渦輪5}
		\caption{\Large 計算結果係數}\label{2.34}
		\end{center}
		\end{figure}
		\\
		計算尺寸(圖.\ref{2.35}) \\
		根據設計參數所生成之渦輪的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{渦輪6}
		\caption{\Large 計算結果尺寸}\label{2.35}
		\end{center}
		\end{figure}
		\\
		建立模型(圖.\ref{2.36}、圖.\ref{2.37}) \\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{渦輪模型}
		\caption{\Large 渦輪模型}\label{2.36}
		\end{center}
		\end{figure}
		\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{渦桿模型}
		\caption{\Large 渦桿模型}\label{2.37}
		\end{center}
		\end{figure}
		\\
\newpage
	\item 齒輪齒條傳動裝置設計器(圖.\ref{2.38}) \\
		齒輪與齒條的代號介紹及尺寸設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{齒輪齒條1}
		\caption{\Large 齒輪齒條傳動裝置設計器}\label{2.38}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.39}) \\
		設定尺條類型及計算條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{齒輪齒條2}
		\caption{\Large 齒輪齒條材質}\label{2.39}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.40}) \\
		齒輪齒條的材質設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{齒輪齒條3}
		\caption{\Large 齒輪齒條材質}\label{2.40}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.41}) \\
		依照參數設定計算出的結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{齒輪齒條4}
		\caption{\Large 齒輪齒條計算結果}\label{2.41}
		\end{center}
		\end{figure}
		\\
		係數(圖.\ref{2.42}) \\
		計算結果的各項係數\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{齒輪齒條5}
		\caption{\Large 計算結果係數}\label{2.42}
		\end{center}
		\end{figure}
		\\
		齒輪參數(圖.\ref{2.43}) \\
		根據設計參數所生成之齒輪與齒條的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{齒輪齒條6}
		\caption{\Large 齒輪齒條材質}\label{2.43}
		\end{center}
		\end{figure}
		\\
		計算尺寸(圖.\ref{2.431}) \\
		根據設計參數所生成之齒輪與齒條的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{齒輪齒條7}
		\caption{\Large 齒輪齒條材質}\label{2.431}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.44}、圖.\ref{2.45}) \\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{齒輪齒條1模型}
		\caption{\Large 齒輪模型}\label{2.44}
		\end{center}
		\end{figure}
		\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{齒輪齒條2模型}
		\caption{\Large 齒條模型}\label{2.45}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 鍊輪設計器(圖.\ref{2.46}) : \\
		鍊條與鍊輪的參數設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{鍊輪1}
		\caption{\Large 鍊輪設計器}\label{2.46}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.47})\\
		設計參數的輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{鍊輪2}
		\caption{\Large 鍊輪細部選項}\label{2.47}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.48})\\
		參數設計的計算分析結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{鍊輪3}
		\caption{\Large 鍊輪計算結果}\label{2.48}
		\end{center}
		\end{figure}
		\\
		計算尺寸(圖.\ref{2.49})\\
		根據設計參數所生成之鍊輪的各細項尺寸，齒輪尺寸有最大與最小值之分\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{鍊輪4}
		\caption{\Large 鍊輪計算結果}\label{2.49}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.50})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{鏈輪模型}
		\caption{\Large 鍊輪模型}\label{2.50}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 壓縮彈簧設計器(圖.\ref{2.51}) :\\
		壓縮彈簧的尺寸設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{壓縮彈簧1}
		\caption{\Large 壓縮彈簧設計器}\label{2.51}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.52})\\
		參數輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{壓縮彈簧2}
		\caption{\Large 壓縮彈簧細部選項}\label{2.52}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.53})\\
		壓縮彈簧材質設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{壓縮彈簧3}
		\caption{\Large 壓縮彈簧材質}\label{2.53}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.54})\\
		根據設計參數所生成之拉伸彈簧的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{壓縮彈簧4}
		\caption{\Large 壓縮彈簧計算結果}\label{2.54}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.55})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{壓縮彈簧模型}
		\caption{\Large 壓縮彈簧模型}\label{2.55}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 拉伸彈簧設計器(圖.\ref{2.56}) :\\
		拉伸彈簧尺寸輸入\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{拉伸彈簧1}
		\caption{\Large 拉伸彈簧設計器}\label{2.56}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.57})\\
		參數輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{拉伸彈簧2}
		\caption{\Large 拉伸彈簧細部選項}\label{2.57}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.58})\\
		拉伸彈簧材料設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=8cm]{拉伸彈簧3}
		\caption{\Large 拉伸彈簧材質}\label{2.58}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.59})\\
		根據設計參數所生成之拉伸彈簧的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{拉伸彈簧4}
		\caption{\Large 拉伸彈簧計算結果}\label{2.59}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.60})\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{拉伸彈簧模型}
		\caption{\Large 拉伸彈簧模型}\label{2.60}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 同步帶輪設計器(圖.\ref{2.61}) :\\
		同步帶輪尺寸輸入\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{同步帶輪1}
		\caption{\Large 同步帶輪設計器}\label{2.61}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.62}) :\\
		參數輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{同步帶輪2}
		\caption{\Large 同步帶輪細部選項}\label{2.62}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.63}) :\\
		計算後結果的各項係數\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{同步帶輪3}
		\caption{\Large 同步帶輪計算結果}\label{2.63}
		\end{center}
		\end{figure}
		\\
		計算尺寸(圖.\ref{2.64}) :\\
		根據設計參數所生成之同步帶輪的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{同步帶輪4}
		\caption{\Large 計算結果尺寸}\label{2.64}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.65}) :\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{同步帶輪模型}
		\caption{\Large 同步帶輪模型}\label{2.65}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 帶輪設計器(圖.\ref{2.66}) :\\
		帶輪的尺寸輸入\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{帶輪1}
		\caption{\Large 同步帶輪模型}\label{2.66}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.67}) :\\
		參數輸入條件\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{帶輪2}
		\caption{\Large 帶輪細部選項}\label{2.67}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.68}) :\\
		設計計算後的分析結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{帶輪3}
		\caption{\Large 帶輪計算結果}\label{2.68}
		\end{center}
		\end{figure}
		\\
		計算尺寸(圖.\ref{2.69}) :\\
		根據設計參數所生成之帶輪的各細項尺寸\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{帶輪4}
		\caption{\Large 計算結果尺寸}\label{2.69}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.70}) :\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{帶輪模型}
		\caption{\Large 計算結果尺寸}\label{2.70}
		\end{center}
		\end{figure}
		\\
		
\newpage
		
	\item 樑設計器(圖.\ref{2.71}) :\\
		樑的尺寸輸入，與軸設計器相同有設計的預覽畫面\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{樑1}
		\caption{\Large 樑設計器}\label{2.71}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.72}) :\\
		樑的材料設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{樑2}
		\caption{\Large 樑材質}\label{2.72}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.73}) :\\
		設計計算後的分析結果，根據圖形選項得到不同的作用力\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{樑3}
		\caption{\Large 樑計算結果}\label{2.72}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.73}) :\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{樑模型}
		\caption{\Large 樑模型}\label{2.73}
		\end{center}
		\end{figure}
		\\
		
\newpage
	\item 柱設計器(圖.\ref{2.74}) :\\
		柱的尺寸輸入\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{柱1}
		\caption{\Large 桿設計器}\label{2.74}
		\end{center}
		\end{figure}
		\\
		選項(圖.\ref{2.75}) :\\
		樑的設計準則\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{柱2}
		\caption{\Large 桿設計準則}\label{2.75}
		\end{center}
		\end{figure}
		\\
		材質(圖.\ref{2.76}) :\\
		樑的材質設定\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=10cm]{柱3}
		\caption{\Large 桿材質}\label{2.76}
		\end{center}
		\end{figure}
		\\
		計算結果(圖.\ref{2.77}) :\\
		設計計算後的分析結果\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{柱4}
		\caption{\Large 桿計算結果}\label{2.77}
		\end{center}
		\end{figure}
		\\
		模型建立(圖.\ref{2.78}) :\\
		\begin{figure}[hbt!]
		\begin{center}
		\includegraphics[width=12cm]{柱模型}
		\caption{\Large 桿模型}\label{2.78}
		\end{center}
		\end{figure}
		\\
\end{itemize}

\newpage


\subsection{同步建模}

\begin{itemize}
\item 隱形參數\\
在看了同步建模的演示後，許多人會認為方向盤的功能太神奇了，透過方向盤的變化，就能將3D模型玩弄於股掌中，不需要輸入參數，就可以隨心所欲地進行模型的設計，但其實這是錯的，畢竟Solid Edge也是由人類藉由科學的力量開發出來的，而不用參數所設計出來的軟體是不存在的，只是用這種方向盤的方式讓參數的變化更自由，而導致有人沒發現而已。\\
以圖\ref{2.79}為例，將方向盤拖動綠色平面往右邊移動時，可以透過選取模型的邊界來讓系統自動確定移動的距離，當從原平面拖移時，系統會檢測且紀錄所選的平面到原始平面位置的距離，這些是由Solid Edge在後台完成，所以說同步建模還是有帶參數的設計過程，只是表面上看不太出來而已。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=10cm]{同步建模演示1}
\caption{\Large 同步建模方向盤}\label{2.79}
\end{center}
\end{figure}
\\
同步建模環境的參數操控比一般建模更加方便且迅速，可以控制參數變化的方向，這比一般建模裡必須依賴約束關係來操控參數變化方向更加的直覺，且能在不需進到草圖的情況下，直接移動模型的邊和面來達到參數的變化，能節省下設計變更的時間。\\
以圖\ref{2.80}為例，如需將30mm的間距修改的話，可以選擇通過移動肋板或是移動外殼的外型來達成，而且只需要簡單的用滑鼠點擊兩方的箭頭即可。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=10cm]{同步建模演示2}
\caption{\Large 同步建模參數變化\\(圖片來源 : 凱德科技股份有限公司)}\label{2.80}
\end{center}
\end{figure}
\\
\item 自由度\\

同步建模可以隨心所欲，彷彿没有規律可循，方向盤利用滑鼠拖移到哪邊，零件就會立即發生相對應的變化，好像就可以隨意設計一樣。但並非這樣的，再拖移方向盤時，Solid Edge系統會時刻檢測操控的動作，同時系統所隱含的設計規則一直在保障模型的變化，只符合和設定的設定規則的模型圖素材會發生變化，在Solid Edge中將此變化稱之為及時規則，這種技術在傳統建模裡是沒有的，這就是同步建模的魅力所在!\\

\item 特徵樹\\

在同步建模的有些操作後，不會發生特徵樹的變化，比如拖動方向盤、旋轉方向盤，但不代表同步建模可以不要特徵樹，同步建模是採納傳統順序建模基於特徵的優勢，並没有抛棄特徵。\\

在傳統順序建模裡，特徵具有嚴格的時間歷史順序，這樣對設計者就提出要求，在設計之前就必須要預先規劃好設計過程，否則就會產生意想不到的结果，而且改了特徵樹前面的草圖及特徵後，後續所有的特徵都需要重新計算，如果不是經驗老到的工程師，能讓規畫好的設計過程來減少更改特徵的次數的話，就會讓導致系統產生大量的等待時間。但特徵樹的好處則在於他會清晰記錄你的所有操作步驟，可以很方便地找到特徵，然後進行修改。\\

同步建模則採納了特徵的這些優勢保留下来，捨棄了它的缺點，就是將歷史的概念去除，讓所有的特徵都在同一層。當在修改其中某一特徵時，所有與之相關的特徵一起發生了變化，而不相關的特徵則保持不變，這樣就極大的提高了效率，且減少了大量的運算時間。 \\

在同步特徵樹下，可以通過按名稱排序，還是類型排序，模型都不會發生任何變化。在拖動方向盤的时候，只不過是對其中部分特徵進行修改，所以也不會再產生新的特徵樹。\\

因此同步建模的特徵模式更加符合設計的特點。\\

\item 特徵參數\\

用多了同步建模方向盤的操作，任何修改都可以用方向盤來實現，而不去修改特徵參數。但其實在同步建模環境中，我們既可以用方向盤的拖曵、旋轉、定向來執行對模型的修改，也可以直接編輯特徵草圖裡的參數，來達成一樣的目標，硬生生地比傳統的建模方式多了一種設計方式。\\

這些特徵在同步建模環境裡統稱為過程特徵，如螺旋拉伸/除料、陣列、孔、螺旋、鈑金中的沖型、補強肋、...。\\

修改圖\ref{2.81}中的陣列，只需要選擇這個特徵，系統就會自動彈出這個特徵的所有參數，如陣列類型、個數、填充方式等，比起傳統建模的修改方式還方便許多。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=10cm]{同步建模演示3}
\caption{\Large 同步建模過程特徵\\(圖片來源 : 凱德科技股份有限公司)}\label{2.81}
\end{center}
\end{figure}
\\
\item 幾何約束\\

習慣了傳統建模的方式，習慣將幾何約束添加在2D草圖，現在在同步建模裡幾何約束也能添加在3D環境下，但同步建模僅僅是有3D的幾何約束，也保留了傳統建模裡的2D幾何約束，各自有各自的工作，名負其責。\\

圖\ref{2.82}是在3D環境下的幾何約束。通過同心、對稱、偏移、共面、相切等等約束關係，直接加載在3D模型的面上，所以說3D約束是控制模型的面。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{同步建模演示4}
\caption{\Large 同步建模3D幾何約束}\label{2.82}
\end{center}
\end{figure}
\\
圖\ref{2.83}是在2D繪圖環境下的幾何約束。雖然功能的名稱是一樣的，但只能加載在2D圖形上，也就是點和線，所以說2D約束是控制2D的點和線，不能控制到3D幾何的面。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{同步建模演示5}
\caption{\Large 同步建模2D幾何約束}\label{2.83}
\end{center}
\end{figure}
\\
兩者相比，因為肉眼能直接看到模型的變化，不須像傳統建模一樣，要先在大腦有個底，所以3D幾何約束比起2D的幾何約束更加直覺、直接地控制模型變化，這是同步建模在CAD建模技術上的一大突破。\\
而同步建模環境，既可以添加2D幾何约束，也能添加3D幾何約束。\\

\end{itemize}
\end{itemize}
\newpage

\subsection{優化算法}
\renewcommand{\baselinestretch}{1}

\begin{itemize}
\item Gradient Descent Optimizer[\ref{OGD}]\\

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=10cm]{Error_surface-2}
\caption{\Large Error surface [\ref{OGD}]}
\label{Error_surface-2}
\end{center}
\end{figure}
%\fontsize{14pt}{28pt}\selectfont
 藉由梯度下降將目標函數值最小化，目標函數以loss function L($\theta$)為例，$\theta$為weight(w)和bias(b)的向量函數，為了找到(圖.\ref{Error_surface-2})上的最小值，因此加上$\Delta\theta$將$\theta$ 的方向修正並引導到正確方向，避免每次修正的過多導致錯過最小值，利用係數$\eta$(學習率)縮放$\Delta\theta$的修正量(圖.\ref{theta_vector})，修正後方程式為：
$$\theta=\theta+\eta\cdot\Delta\theta$$
\begin{figure}
\begin{center}
\includegraphics[width=10cm]{theta_vector}
\caption{\Large theta vector[\ref{OGD}]}
\label{theta_vector}
\end{center}
\end{figure}
\newpage
將$\theta$以泰勒展開式表示，假設並$\Delta\theta$為u:
\begin{center}
$L_{(\theta+\eta u)}=L_{(\theta)}+\eta u^{T}\cdot\bigtriangledown_{\theta} L_{(\theta)}+\frac{\eta^2}{2!}u^T\cdot\bigtriangledown^2 L_{(\theta)}u+\frac{\eta^3}{3!}...+\frac{\eta^4}{4!}...+\frac{\eta^n}{n!}...$\\
\end{center}
以泰勒展開式的型式表示的好處是：$\theta$些微的更動產生新值。$\eta$值通常小於一，當$\eta^2 << 1$，因此可以忽略高階項 
\begin{center}
$L_{(\theta+\eta u)}=L_{(\theta)}+\eta u^{T}\cdot\bigtriangledown_{\theta} L_{(\theta)} [\eta\ is\ typically\ small, so\ \eta^2, \eta^3,\cdots \rightarrow 0]$\\
\end{center}
新的$L(\theta + \eta u)$輸出的值會小於$L(\theta) L(\theta+\eta u) − L(\theta) < 0$，同理可證$u^T\cdot\bigtriangledown\theta L(\theta)$，符合u這條件：當新的值小於舊的值，u就是一個好的值。假設u和$\bigtriangledown\theta L(\theta)$的夾角為$\beta$\\
\begin{center}
$\cos(\beta)=\frac{u^{T}\cdot\bigtriangledown_{\theta} L_{(\theta)}}{\vert u^{T}\vert\vert \bigtriangledown_{\theta} L_{(\theta)}\vert}$\\
\end{center}
因為$\cos(\theta)$的值介於1和-1之間\\
\begin{center}
$-1<\cos(\beta)=\frac{u^{T}\cdot\bigtriangledown_{\theta} L_{(\theta)}}{\vert u^{T}\vert\vert \bigtriangledown_{\theta} L_{(\theta)}\vert}\leq 1$\\
$k=\vert u^{T}\vert\vert \bigtriangledown_{\theta} L_{(\theta)}\vert$\\
$-k \leq k\cos(\beta)=u^{T}\cdot\bigtriangledown_{\theta} L_{(\theta)}\leq k$
\end{center}
\newpage
 所以盡可能的讓新值小於舊值$(L(\theta+\eta u) − L(\theta) < 0)$，loss 值就會減少得越多。因此$u T \cdot \bigtriangledown\theta L(\theta)$應該為負，在這情況下$\cos(\beta)$於−1，$\beta$的角度為 $180^{\circ}$這就是$\theta$移動的方向與梯度方向相反的原因。 梯度下降法告訴我們：當$\theta$在特定值，並想減少新的$\theta$值，使 loss 值逐漸減少就應該與梯度相反的方向找 (若梯度為正值，找最小值就需往負的方向找):
\begin{center}
$w_{t=1}=w_t-\eta\bigtriangledown w_t$\\
$b_{t=1}=b_t-\eta\bigtriangledown b_t$\\
$where\ at\ w=w_t,b=b_t$\\
$\begin{cases}
 \bigtriangledown w_t=\frac{\partial L_{_{(\theta)}}}{\partial w}
 \bigtriangledown b_t=\frac{\partial L_{_{(\theta)}}}{\partial b}
 \end{cases}$
\end{center}
\item Batch gradient descrnt [\ref{OGD2}]\\
 Vanilla gradient descent 又稱 Batch gradient descent(批次梯度下降法)，計算目標函數的梯度，參數$\theta$對於整個 訓練資料：
\begin{center}
$\theta=\theta-\eta\cdot\bigtriangledown_{\theta}L_{(\theta)}$
\end{center}
 目標函數以為例 loss function L($\theta$)，參數$\theta$為 weight(w)和 bias(b)的函數，$\eta$為學習率。由於計算整個資料集計算梯度只更新一次，Bath gradient descent 可能非常慢並且對於資料集無法符合及記憶體來說棘手(一次需要儲存整個資料集的資料，當更新和計算時會占用大量記憶體)。\\
\begin{lstlisting}[caption=\Large Batch gradient descrnt]
 for i in range(nb_epochs) :
params_grad = evaluate_gradient (loss_function, data, params)
params = params − learning_rate * params_grad
\end{lstlisting}
\newpage
 預定義每次epoch，先計算loss function梯度向量對於整個資料集參數向量。如果梯度值來自於先前計算出的梯度值，就會檢查梯度，並以梯度相反的方向更新參數$\theta$，學習率$\eta$決定多大的更新量。Batch gradient descent對於凸面誤差可以保證收斂到廣域最小值，對於非面凸誤差可以收斂到局部最小值。
\item Stochastic gradient descent(SGD)[\ref{OGD2}]\\
隨機梯度下降法，這裡的目標函數為$J(\theta, x^i, y^i)$(變數$\theta$為 w(weight) 和 b(bias) 的函數，也可以寫成$J(w,b,\theta, x^i, y^i)$)。\\
\begin{center}
$\theta=\theta-\eta\cdot\bigtriangledown_{\theta}J_{(\theta, x^i, y^i)}$
\end{center}
批量梯度下降他會在每個參數更新前重新計算相似梯度。SGD 每次次執行會更新來消除多餘 (誤差)，因此通常速度 很快。SGD 頻繁更新並變化很大，因為目標方程式波動很大(圖.\ref{sgd_fluctuation})。\\

\begin{figure}
\begin{center}
\includegraphics[width=10cm]{sgd_fluctuation}
\caption{\Large SGD fluctuation[\ref{OGD2}]}
\label{sgd_fluctuation}
\end{center}
\end{figure}
 SGD 的方程式一方面會跳到新的值和潛在局部最小值，另一方面 SGD 會持續超調 (誤差超過預期) 最後收斂到廣域最小值。無論如何他被顯示當學習率下降緩慢，SGD 顯示與 Batch gradient descent 同樣收斂行為，幾乎可以肯定地，對於凸面或非凸面優化，會收斂到絕對或是局部最小值。這程式碼片段[程式.2.2] 在訓練樣本上加入一個迴圈來對每個樣本評估梯度。每個 epoch(訓練循環) 會打亂訓練數據。
\label{code Stochastic gradient descrnt code}
\begin{lstlisting}[caption=\Large Stochastic gradient descrnt ]
for i in range(nb_epochs) :
np.random.shuffle(data)
for example in data :
params_grad = evaluate_gradient (loss_function, example, params)
params = params − learning_rate * params_grad
\end{lstlisting}
\item Mini-batch gradient descent[\ref{OGD2}]

 Mini-batch gradient descent(小批量梯度下降) 各取前兩者的優點，將資料集分割成小區塊，每個小區塊大小稱作 batch size，每次跑完 batch size 算迭代 (iteration)一次，算完一次資料集即完成一次 epoch。舉例: 資料集大小為1000，若 batch size 為50，iteration 為 datasets 的batch size  = 1000÷50 = 20，當 iteration 跑完 20 次算完成一次 epoch。\\
這方式可以減少參數更新的方差，並且可以穩定收斂；可利用深度學習庫所共有的高度優化的矩陣優化，從而由一個小批量計算出梯度非常有效。通常 batch sizes 的範圍介於 50 ~256，會因為應用而有所差異。訓練神經網絡時，通常選擇 Mini-batch gradient descent 算法，而當使用這算法時，通常也用 SGD 稱呼。\\
\begin{center}
$\theta=\theta-\eta\cdot\bigtriangledown_{\theta}J_{(\theta, x^{(i:i+n)}, y^{(i:i+n)})}$
\end{center}
下面[程式.2.3] 為迭代範例，batch size 大小為 50：\\
\label{Mini-batch gradient descrnt}
\begin{lstlisting}[caption=\Large Mini-batch gradient descrnt]
for i in range(nb_epochs ) :
np.random.shuffle(data)
for batch in get_batches (data, batch_size = 50):
params_grad = evaluate_gradient (loss_function, batch, params)
params = params − learning_rate ∗ params_grad
\end{lstlisting}
Mini-batch gradient descent 無論如何還是無法確保收斂的很好，存在一些需要解決的挑戰：
\begin{enumerate}[1]
\item 選擇適當的學習率是有難度的。如果學習率太小會導致收斂困難或緩慢，學習率太大則會阻礙收斂導致 loss function 來回波動或發生偏離。
\item 學習率清單嘗試在訓練的時候調整學習率，即根據預定義清單或當目標下降於閾值 (threshold) 時降低學習率。 但清單和閾值須預先定義，因此無法適應數據集的特徵。
\item 另外相同學習率適用全部參數更新。如果資料稀疏而且外型有很特別的頻率，我們可能不希望將所有特徵更新 到相同的程度，而是對很少發生的特徵執行較大的更新。
\item 最小化神經網路常見的高度非凸面誤差方程式 (error function) 的另一關鍵挑戰則是要避免被困在大量次優的局 部最小值區域中。認為困難實際上不是由局部最小值引起的，而是由鞍點引起的，即一維向上傾斜而另一維向下 傾斜的點。這些鞍點通常被相同誤差的平穩段包圍，這使得 SGD 很難逃脫，因為在所有維度上梯度都接近於零。
 \end{enumerate}
\item Gradient descent optimization algorithms[\ref{OGD2}]\\
SGD 難以在陡峭的往正確的方向，那就是說在一個維度上，曲面的彎曲比另一個維度要陡得多，這在局部最優情況下很常見。下圖(圖.1)的同心圓代表中心下凹的曲面。在這些情況下，SGD 會在陡峭的地方振盪，而僅沿著底部朝著局部最優方向猶豫前進，如(圖.\ref{fig.without_momentum}) 所 示。 Momentun(動量) 是一個幫助加速 SGD 在正確方向和抑制震盪的方法，在(圖.\ref{fig.with_momentum})。\\

\begin{figure}[hbt!]
\begin{center}
\subfigure{
\begin{minipage}[t]{0.5\linewidth}  %設定圖片間距
\includegraphics[width=6cm]{without}
\caption{\Large SGD without momentum[\ref{OGD2}]}
\label{fig.without_momentum}
\end{minipage}
}
\subfigure{
\begin{minipage}[t]{0.5\linewidth}
\includegraphics[width=6cm]{with}
\caption{\Large SGD with momentum[\ref{OGD2}]}
\label{fig.with_momentum}
\end{minipage}
}
\end{center}
\end{figure}

這麼做會增加一個係數$\gamma$來更新上次的向量到正確向量 (修正偏差)，$\gamma$通常設為 0.9 左右。
\begin{center}
$v_t = \gamma v_{t-1}+\eta\cdot\bigtriangledown_{\theta}J_{(\theta)}$
$\theta = \theta-v_t$
\end{center}
 實際上，使用動量的時候，就像將球推下山坡。球在下坡時滾動時會累積動量，在途中速度會越來越快（如果存在空氣阻力，直到達到極限速度，也就是$\gamma < 1$) 參數更新也發生了同樣的事情：動量 (momentum) 對於梯度指向相同方向的維度增加，而對於梯度改變方向的維減少動量。結果，我們獲得了更快的收斂並減少了振盪。\\

 Nesterov accelerated gradient（NAG）是一種使動量具有一個去向的概念，以便在山坡再次變高之前知道它會減速。我們知道使用動量$\gamma v_{t-1}$來移動參數。計算$\theta - \gamma v_{t-1}$這樣就給了參數的下一個位置的近似值（完整更新缺少的梯度），這是參數將要存在的大致概念。現在，通過計算與當前參數無關的梯度來有效地看到目前的參數$theta$將會移動到的位置：
\begin{center}
$v_t = \gamma v_{t-1}+\eta\cdot\bigtriangledown_{\theta}J_{(\theta-\gamma v_{t-1})}$
$\theta = \theta - v_t$
\end{center}

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=13cm]{NAG}
\caption{\Large NAG[\ref{OGD2}]}
\label{NAG}
\end{center}
\end{figure}
同樣，我們設置動量$\gamma$約為0.9。動量首先計算當前梯度（(圖.\ref{NAG})中的藍色小向量），然後在更新的累積梯度（藍 色向量）的方向上發生較大的跳躍，而 NAG 首先在先前的累積梯度的方向上進行較大的跳躍（棕色向量），測量梯度，然後進行校正（紅色向量），從而完成 NAG 更新（綠色向量）。這種預期的更新可防止我們過快地進行，並導致響應速度增加，從而顯著提高了 RNN 在許多任務上的性能。\\
Adagrad[\ref{OGD2}]：\\
 Adagrad 是一個梯度優化的算法，它可以做到：學習率適應參數，對於頻繁出現的特徵相關參數執行較小的更新(較低的學習率)，以及對不經常出現的特徵相關參數進行較大更新（即學習率較高）。Adagrad可以提高SGD的強度，用於訓練大型神經網絡。\\
 先前，在同一次$theta$參數(更新後就算另一次)，每個$theta$都使用相同的$\eta$(學習率)。Adagrad 則是對每個$theta$參數使用 不同的$\eta$，t 代表 time step。先將 Adagrad 的更新參數向量化。用$g_t$表示目標函數 (參數$theta$在 time step t) 對參數做偏微分計算。
\begin{center}
$g_{t,i}=\bigtriangledown_{\theta}J_{(\theta_{t,i})}$
\end{center}
當 SGD 更新每個參數$\theta_i$，在每個 time step t，因此變成：
\begin{center}
$\theta_{t+1,i}=\theta_{t,i}-\eta\cdot g_{t,i}$
\end{center}
更新規則，Adagrad 根據先前$\theta_i$計算的梯度，對每個參數$\theta_i$修改整個學習率$\eta$在每個 time stept：
\begin{center}
$\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}$
\end{center}
$G_t \in \mathbb{R}^{d\times d}$這是一個對角矩陣每個對角元素 i，i 是關於$theta$梯度平方和取決於 time stept，$\epsilon$是避免分母為0($\epsilon$通常為$10^{-8}$)，如果沒有平方根運算，該算法的性能將大大降低。$G_t$包含了過去梯度平方根，由於全部$theta$參數沿著對角線，通過向量的內積計算$G_t$和$g_t$:
\begin{center}
$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}}\cdot g_t$
\end{center}
 Adagrad 主要好處之一是，無需手動調整學習率。大多數實現使用預設值 0.01 並將其保留為預設值。Adagrad主要弱點是會累積分母的平方梯度：由於每項都是正的，累積和會在訓練中不斷增長。反過來，學習率下降，並最終變得無限小，這算法就不再獲得知識。\\
Adadelta[\ref{OGD2}]：\\
 Adadelta是Adagrad的延伸，下降其激進的程度，單調的降低學習率。Adadelta會限制過去累積的梯度，並將其限制在某個特定大小 w，並代替Adagrad過去累積的梯度平方，以梯度總和是遞迴定義為所有過去衰減梯度平方平均值。流動平均$E[g^2]_t$ 在time stept然後取決於(像Momentum的$\gamma$)先前平均和最近梯度：
\begin{center}
$E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g^2 _t$
\end{center}
$\gamma$值和 Momentum 的相似，約為 0.9，現在根據參數更新向量$\bigtriangleup\theta_t$來重寫 SGD：
\begin{center}
$\bigtriangleup\theta_t=-\eta\cdot g_{t,i}$\\
$\theta_{t+1}=\theta_t+\bigtriangleup\theta_t$
\end{center}
Adagrad 的參數更新向量替換成：對角矩陣$G_t$過去梯度平方的衰退平均$E[g^2]_t$
\begin{center}
$\bigtriangleup\theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}}\cdot g_t$\\
$replace\ G_t\ with\ E[g^2]_t\Rightarrow\bigtriangleup\theta_t=-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\cdot g_t$
\end{center}
由於分母只是梯度的均方根 (RMS)，我們可以取代成縮寫：
\begin{center}
$\bigtriangleup\theta_t=-\frac{\eta}{RMS[g]_t}\cdot g_t$
\end{center}
 這個更新單位和 SGD、Momentum 以及 Adagrad 的單位不符合，因此更新需有相同的參數。為了實現這一點，首先定義另一個指數衰減平均值，這次不是梯度平方更新而是參數平方更新：
\begin{center}
$E[\bigtriangleup\theta^2]_t=\gamma E[\bigtriangleup\theta^2]_{t-1}+(1-\gamma)\bigtriangleup\theta^2 _t$
\end{center}
RMS 參數更新:
\begin{center}
$RMS[\bigtriangleup\theta]_t=\sqrt{E[\bigtriangleup\theta^2]_t+\epsilon}$
\end{center}
$RMS[\bigtriangleup\theta]_t$是未知的，更新參數的 RMS 取近似值到上個 time step。用$RMS[\bigtriangleup\theta]_t$取代學習率$\eta$，最後產生新的規則：
\begin{center}
$\bigtriangleup\theta_t=-\frac{RMS[\bigtriangleup\theta]_{t-1}}{RMS[g]_t}g_t$
\\
$\theta_{t+1}=\theta_t+\bigtriangleup\theta_t$
\end{center}
使用 Adadelta，甚至不需要設定預設學習率，因為它已從更新規則淘汰。\\
RMSprop[\ref{OGD2}]：\\
 RMSprop 是 Geoffrey Hinton 在他的課程中提出的未公開自適應學習率的方法。\\

 RMSprop 和 Adadelta 都是為了解決 Adagrad 的學習率急劇下降的問題個別獨立開發出來的解決方式。RMSprop 實際上與 Adadelta 得出的第一個更新向量相同：
\begin{center}
$E[g^2]_t=0.9E[g^2]_t+0.1g^2 _t$
\\
$\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t$
\end{center}

 RMSprop 也將學習率除以梯度平方的指數衰減平均值。Hinton 建議$\gamma$設為 0.9，好的預設學習率$\gamma$數值為 0.001。\\
Adaptive Moment Estimation：[\ref{OGD2}]\\
 Adaptive Moment Estimation 自適應矩評估 (Adam) 是另一種計算每個評估學習率的方法。出了儲存過去梯度平 方的指數衰減平均值$v_t$，就像 Adadelta 和 RMSprop 一樣，Adam 還保留過去梯度的指數衰減平均值$m_t$，類似動量 (Momentum)。如果 Momentum 被視為順著斜坡下滑的球，而 Adam 則是像一個帶有摩擦的沉重的球，因此更適合待在 error face 平坦的最小值區域。計算過去梯度平方的衰減平均值$m_t$和$v_t$分別如下：
\begin{center}
$m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t$\\
$v_t=\beta_2 v_{t-1}+(1-\beta_2)g^2 _t$
\end{center}
$m_t$和$v_t$分別是第一階矩平均估計值和第二階矩無中心方差估計值，因此是方法的名稱。像$m_t$和$v_t$被初始化為向量 o，Adam 的作者觀察到它們偏向零，特別是在初始 time step，尤其是在衰減率較小的時候 (也就是說$\beta_1$和$\beta_2$趨近於 1) 藉由計算校正偏差第一矩$\hat{m}_t$和第二矩$\hat{v}_t$抵消偏差：
\begin{center}
$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}$\\
$\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}$
\end{center}
使用他們去更新參數，就像 Adadelta 和 RMSprop 中所看到的那樣，這將產生 Adam 更新規則：
\begin{center}
$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$
\end{center}
$\beta_1$預設值建議為 0.9，$\beta_2$預設值建議為 0.999，ϵ 預設值建議為$10^{-8}$。根據經驗證明 Adam 表現良好，並且與其他自適應學習算法相比具有優勢。
在 Adam 更新規則中的$v_t$係數是與梯度成反比地縮放過去梯度的範數 (通過 $v_{t-1}$項) 和當前梯度$|g_t|^2$：
\begin{center}
$v_t = \beta_2 v_{t-1} + (1 - \beta_2) |g_t|^2$
\end{center}
我們轉換這個更新到$\ell_p$。注意$\beta_2$參數化為$\beta_2^p$：
\begin{center}
$v_t = \beta_2^p v_{t-1} + (1 - \beta_2^p) |g_t|^p$
\end{center}
大規範 p 值使數值上變得不穩定，這就是為什麼$\ell_1$和$\ell_2$規範在實踐中是最常見的。然而$\ell_\infty$通常也表現出穩定 的行為。為了避免與 Adam 混用，所以使用$u_t$來表示無窮範數約束$v_t$：
\begin{center}
$u_t = \beta_2^\infty v_{t-1} + (1 - \beta_2^\infty) |g_t|^\infty$\\
$= \max(\beta_2 \cdot v_{t-1}, |g_t|)$
\end{center}
替換為Adam更新公式$\sqrt{\hat{v}_t} + \epsilon$和$u_t$得出AdaMax更新規則：
\begin{center}
$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{u_t} \hat{m}_t$
\end{center}
替換為Adam更新公式$\sqrt{\hat{v}_t} + \epsilon$和$u_t$得出AdaMax更新規則：
\begin{center}
$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{u_t} \hat{m}_t$
\end{center}
注意$u_t$依靠最大運算，不建議 Adam 中的$m_t$和$v_t$偏向零，這就是為什麼不需要針對$u_t$計算偏差。好的預設值$\eta = 0.002$ $\beta_1 = 0.9$ 和 $\beta_2 = 0.999$。\\
\end{itemize}


\section{強化學習}
%=----------What is Reinforcement Learning?------------=%
強化學習(Reinforcement Learning，簡稱為RL)是通過agent(代理)與已知或未知的環境持續互動，不斷適應與學習，會得到正向或負面的回饋，對應到獎賞(reward)和懲罰(punishments)。考慮到agent與環境(environment)互動，進而決定要執行哪個動作，強化學習的學習模式是建立在獎賞與懲罰上。\\

強化學習與其他學習法不一樣的地方在於：不需要事先收集大量數據提供當作學習樣本，而是透過與環境互動，在環境下發生的狀態當作學習的資料來源，透過不斷嘗試使所得到的獎勵最大化。其他類型的機器學習大都需要給予特定資料且有明確的答案。\\

由於強化學習是建立在agent與環境互動上，因此許多參數進行運算，需要大量資訊來學習，並根據資訊採取行動。強化學習的環境可以是真實世界、2D或3D模擬世界的場景。強化學習的範圍很廣，因為環境的規模可能很大，且在環境中有多相關因素，影響著彼此。強化學習以獎勵的方式，促使學習結果趨近或達到目標結果。\\
%=----------Faces of Reinforcement Learning---------------=%

強化學習涵蓋範圍(圖.\ref{各領域與機器學習應用範圍})：\\
 強化學習可以運用在計算機科學、神經科學、心理學、經濟學、數學、工程等領域，涵蓋領域相當廣泛。
%======需文字補充========%
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=11cm]{Faces_of_Reinforcement_Learning}
\caption{\Large 各領域與機器學習應用範圍}
\label{各領域與機器學習應用範圍}
\end{center}
\end{figure}
%=--------The Flow of Reinforcement Learning------------=%.
\newpage
強化學習的流程：\\
透過agent與環境間互動而產生狀態和獎勵，由於狀態的轉移，agent會決定接下來執行動作(圖.\ref{RL structur})。\\[12pt]

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{The_Flow_of_Reinforcement_Learning}
\caption{\Large 強化學習架構}
\label{RL structur}
\end{center}
\end{figure}
需要考慮的重點：\\
強化學習的狀態、獎勵和動作是互相關聯，agent與環境之間存在著關聯，兩者都影響著狀態和動作並互相影響著彼此：機器人會因動作而造成狀態轉移，狀態的移轉也會影響機器人做出的決策。\\
\newpage
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{The_entire_interaction_process}
\caption{\Large 整個互動過程}
\label{整個互動過程}
\end{center}
\end{figure}
 如(圖.\ref{整個互動過程}) agent會透過輸入的狀態來決定採取何種行為(動作)，並試圖採取獲得最高獎勵的行動。當agent開始與環境互動時，agent會透過當前狀態來決定將採取的行動，在agent採取行動後，環境的狀態也因此而改變，若agent採取行動後所到達我們所要的狀態就會得到獎勵，反之則會給予懲罰。在場景裡透過反覆的訓練，讓強化學習的行為漸漸趨近預期的目標。\\
%=----Different Terms in Reinforcement Learning----------=%
 強化學習中有兩個很重要的常數：$\gamma$和$\lambda$。\\
 
$\gamma$會影響所獲得的獎勵。$\gamma$又稱為衰減因子，正常狀態下為小於1的常數用於每個狀態改變，當狀態改變時為時常數。$\gamma$允許使用者在每個狀態給予不同形式的獎勵(這種狀況下$\gamma$為0)，如果著重在長期的決策時，獎勵就不受決策順序所影響(此時$\gamma$為1)\\

$\lambda$一般在我們處理時間差異問題時使用。 這是涉及更多地連續狀態的預測。在每個狀態中$\lambda$值的增加代表演算法正在快速學習。\\
%------------------圖片可共用----------------------%
\iffalse
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.74]{ Reinforcement_Learning_interactions}
\caption{\Large Reinforcement Learning interactions}
\end{center}
\end{figure}
\fi
%=----Interactions with Reinforcement Learning------------=%

強化學習的互動是透過agent和環境之間的互動會產生獎勵，agent採取行動，導致狀態改變是一種強化學習實現如何將情況映設為行動的方法，從而找到最大化獎勵的方法，機器或機器人不會像其他機器學習形式的機器人那樣被告知要採取哪些行動。\\

獎勵的目的與運作以獎勵的方式誘導機器採取我們所期望的動作，機器會採取最大化獎勵的方式，因此可將目的定為最大獎勵，以吸引機器執行期望做的行為。\\

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{agent}
\caption{\Large agent}
\label{agent}
\end{center}
\end{figure}
%=----Agents------------=%
\begin{flushleft}
強化學習的環境：\\
\end{flushleft}
強化學習中的環境由某些因素組成，會對agent產生影響，agent必須根據環境適應各種因素，並做出最佳決策，這些環境可以是各種形式，其中包括2D、3D或是真實世界。強化學習的環境具有確定性、可觀察性，可以是離散或是連續的狀態，則agent可以是單一或多個所組成。\\
\subsection{馬可夫決策}
\begin{itemize}
\item Markov Chain\\
 馬可夫鏈(Markov Chain)主要是狀態變化的隨機過程(stochastic process)和馬可夫屬性(Markov property)結合。隨機過程(stochastic process)狀態隨著時間變化，而狀態的變化存在著雖機性，並以數學模式表示。馬可夫屬性(Markov property)指在目前以及所有過去事件的條件下，任何未來事件發生的機率，和過去的事件不相關僅和目前狀態相關。當前決策只會影響下個狀態，當前狀態轉移(action)到其他狀態的機率會有所差異。\\
\item Markov Reward Process\\
\begin{itemize}
\item action 到指定狀態會獲得獎勵。
$$R(s_t=s) = \mathbb{E}[r_t|s_t = s]$$
$$\gamma \in [0, 1]$$
\item Horizon：
在無限的狀態以有限的狀態表示。
\item Return：
越早做出正確決策獎勵越高。
$$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+...+\gamma^{T-t-1} R_{T}$$
\item State value function(決策價值)：
$$V_t(S) = \mathbb{E}[G_t|s_t = s]$$
$$P(s_{s+1}=s'|s_t=s,a_t=a)$$
\end{itemize}
\item Discount Factor ($\gamma$)
獎勵衰減有幾種作法：第一種，越早做出有獎勵的決策，獎勵越高：第二種，做出有價值的決策$\gamma = 1$，不分決策順序先後；第三種，無用的決策$\gamma = 0$，不會得到獎勵。\\
以Bellman equation的方式描述互動關係狀態：\\
$$V(s) = R(s)+\gamma\sum_{s'\in S}P(s'|s)V(s')$$
\begin{center}
$R(s)$:立即獎勵\\
$\gamma\sum_{s'\in S}P(s'|s)V(s')$：未來獎勵衰減總和
\end{center}
Anaytic solution(分析性解法)，MRP的分析性解法：
$$V = (1-\gamma P)^{-1}R$$
Bellman equation及Anaytic solution的方式只適合小的MRP(個數比較少的)，矩陣複雜度為$O(N^3)$，N為狀態個數。若要計算大型的MRP會使用疊代法：動態規劃(Dynamic programming)、Temporal-Difference learning和Mote-Carlo evaluation以評估採樣的方式：
$$g = \sum_{i=t}^{H-1}\gamma^{1-t}r_i$$
$$G_t \leftarrow G_t+g,  i \leftarrow i+1$$
$$V_t(s) \leftarrow \frac{G_t}{N}$$
\item Markov Decision Process在MRP中加入決策(decision)和動作(action)
\begin{itemize}
\item S：state 狀態
\item A：action 動作
\item P：狀態轉換
$P(s_{s+1}=s'|s_t=s,a_t=a)$
\item R：獎勵，取決於當前狀態和動作會得到相對應的講勵
$$R(s_t=s, a_t=a) = \mathbb{E}[r_t|s_t, a_t=a]$$
\item D：折扣因子(discount factor)
$$\gamma \in [0,1]$$
\end{itemize}
\end{itemize}

\begin{flushleft}
policy(決策)：可以是一個決策行為的機率或確定執行的行為，若以數學方程式表示：
$$\pi (a|s) = P(a_t=a|s_t=s)$$
\newpage

MRP和MDP方程式互相轉換：\\
\end{flushleft}
%=========表格=========%
\begin{center}
\begin{tabular}[c]{ccc}    
%\multicolumn{1}{r}{MRP}
 MRP & $\longleftrightarrow$ & MDP\\
\hline
$P^{\pi}(s's)$ & = & $\sum_{a\in A}\pi (a|s)P(s'|s, a)$\\
$P^{\pi}(s)$ & = & $\sum_{a\in A}\pi (a|s)P(s, a)$\\
\includegraphics[height=3cm]{MRP}&&\includegraphics[height=3cm]{MDP}\\
\hline
\end{tabular}
\end{center}
\hspace{15pt}
 
state value function(狀態值方程式)$v^{\pi}(s)$\\
$$v^{\pi}(s) = \mathbb{E}[G_t|s_t=s]$$
$$= \mathbb{E}[R_{t+1}+\gamma v^{\pi}(s_{t+1})|s_t=s]$$
$$= \sum_{a\in A}\pi (a|s)q^{\pi}(s, a)$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=8cm]{s_to _s}
\caption{$v^{\pi}$程序圖}
\label{fig.s_to_s}
\end{center}
\end{figure}
$$v^{\pi}(s) = \sum_{a\in A}\pi (a|s)(R(s, a)+\gamma \sum_{s'\in s}P(s'|s, a)v^{\pi}(s'))$$
\newpage
state value function(狀態值方程式)$q^{\pi}(s)$
$$v^{\pi}(s) = \mathbb{E}[G_t|s_t=s]$$
$$= \mathbb{E}[R_{t+1}+\gamma v^{\pi}(s_{t+1})|s_t=s]$$
$$= \sum_{a\in A}\pi (a|s)q^{\pi}(s, a)$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=8cm]{Q_pi function}
\caption{$q^{\pi}$程序圖}
\label{fig.q_pi}
\end{center}
\end{figure}
$$q^\pi(s, a)=R(s, a)+\gamma\sum_{s'\in S}P(s'|s, a)\sum_{a'\in A}\pi(a'|s')q^{\pi}(s', a')$$
\newpage
\section{Policy Gradient理論}
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{policy gradient原理}
\caption{\Large Policy Gradient原理}
\label{Policy Gradient原理}
\end{center}
\end{figure}

Policy Gradient主要目的是直接對決策進行建模與優化。該決策(policy)通常使用參數化函數建模，獎勵（目標）函數的值取決於此決策，可以應用各種算法來優化，以獲得最佳獎勵。(參數化：當軟體建置於一給定環境時，再依該環境的實際需求填選參數，即可成為適合該環境。)\\
參數介紹[\ref{R.Policy Gradient}]:\\
$\pi$：policy\\
s：狀態(States)。\\
a：動作(Actions)。\\
r：獎勵(Rewards)。\\
$S_t,A_t,R_t$：一個軌跡時間步長't'的State,Action and Reward。 \\
$\gamma$：衰減因子(Discount Factor);懲罰未來的不確定獎勵(reward)。\\
$G_t$：回傳衰減後的未來獎勵(Discounted future reward)$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$\\
$P(s', r \vert s, a)$：伴隨著現在狀態(state)的a和r，前往下一個狀態s'的轉移機率矩陣(單階)。\\
$\pi(a \vert s)$：隨機策略(agent的行為策略)。\\
$\mu(s)$：確定的策略;我們還使用不同的字母將其標記為$ \pi（s）$，以提供更好的區分，以便我們可以輕鬆判斷策略是隨機的還是具有確定性的\\
$V(s)$：'狀態值函數'測量狀態的預期收益(報酬率)\\
$V^\pi(s)$：根據policy的狀態值函數$V^\pi (s) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s]$\\
$Q(s, a)$：行為值函數，評估一對狀態和動作的預期收益。\\
$Q^\pi(s, a)$：根據policy的行為值函數$Q^\pi(s, a) = \mathbb{E}_{a\sim \pi} [G_t \vert S_t = s, A_t = a]$。\\
$A(s, a)$：Advantage Function，$A(s, a) = Q(s, a) - V(s)$：像是另一種版本的Q-value，由狀態值為基準降低方差。\\
reward function的值：取決於策略，可應用各種算法優化$\theta$，獲得最佳獎勵。\\[5pt]
$$J(\theta) 
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s) 
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)$$\\

Policy Gradient 通過反覆評估梯度來最大化預期的總獎勵(reward)\\[5pt]
$g = \nabla_\theta\mathbb{E}[\sum_{t=0}^\infty r_t]$ ; $g = \mathbb{E}[\sum_{t=0}^\infty\psi_t\nabla_\theta log\pi_\theta(a_t \vert s_t)]$\\[5pt]
\begin{Large}{$\psi_t$ 可能方法為下列:}\end{Large}
\begin{itemize}
\item $\sum_{t=0}^\infty r$：決策軌跡的獎勵總和。
\item $\sum_{t^{'}=t}^\infty r^{'}$: 根據動作(action)的獎勵(reward) $a_t$。\\
標準表示式：$\sum_{t^{'}=t}^\infty r_t^{'}-b(s_t)$
\item $Q^\pi(s_t,a_t)$：state-action value function。
\item $A^\pi(s_t,a_t)$：Advantage Function。
\item $r_t+V^\pi(s_t+1)-V^\pi(s_t)$：TD residual。
\end{itemize}
公式使用定義[\ref{R.Policy Gradient}]：\\[5pt]
$V^\pi(s_t) = \mathbb{E}_{s_{t}+1:\infty,a_{t}:\infty}[\sum_{l=0}^\infty r_t+l]$\\[5pt]
$Q^\pi(s_t,a_t) = \mathbb{E}_{s_{t}+1:\infty_,a_{t}+1:\infty}[\sum_{l=0}^\infty r_t+l]$\\[5pt]
$A^\pi(s_t,a_t) = Q^\pi(s_t,a_t)-V^\pi(s_t)(Advantage Function)$\\[5pt]

\subsection{Actor Critic}
原始的policy gradient沒有偏差，但方差大;所以提出了許多以下算法來減少方差，同時保持偏差不變：\\[5pt]
$$g = \mathbb{E}[\sum_{t=0}^\infty\psi_t\nabla_\theta log\pi_\theta(a_t \vert s_t)]$$\\[5pt]
Actor-Critic：減少原始政策中的梯度方差包括兩個模型\\[5pt]
Critic：更新值函數參數w，根據算法，它可以是操作值$ Q_w$（$a \vert s$）或狀態值$V_w$（$s$） \\[5pt]
Actor：按照Critic的建議，將策略參數 $\theta$ 更新為 $\pi_\theta$（$a \vert s$）\\[5pt]
\begin{Large}
它如何在簡單的行動價值參與者批評中發揮作用:
\end{Large}
\begin{itemize}
\item 隨機的初始化 s,$\theta$,w ;取樣 a $\sim
\pi_\theta(a \vert s)$
\end{itemize}
\begin{itemize}
\item For $t =1 \sim T:$ 
\begin{enumerate}[1]
\item 取樣 reward $r_t$ $\sim$ $R(s,a)$ 隨後下一階段 $s'$ $\sim$ $P(s'\vert s,a)$ 
\item 樣本的下一個動作 $a' \sim$ $\pi_\theta(a'\vert s')$
\item 更新 policy 參數 $\theta$ :\\
$$\theta\leftarrow\theta+\alpha_\theta Q_w(s,a)\nabla_\theta ln\pi_\theta(a\vert s)$$
\item 計算校正 (TD error)對於時間t的動作值:\\
$$\delta = r_t + \gamma Q_w(s',a')-Q_w(s,a)$$
並使用它來更新操作action - value function:\\
$$w\leftarrow w+\alpha_w \delta \nabla_w Q_w(s,a) $$
\item 更新 $a\leftarrow a'$ 和 $ s \leftarrow s'$ ; 學習率：$a_\theta$ 和 $a_w$。
\end{enumerate}   
\end{itemize}\newpage

\iffalse
\subsection{Policy Gradient Theorem}
Policy Gradient 通過反覆估計梯度來最大化預期的總reward\\[5pt]
$g = \nabla_\theta\mathbb{E}[\sum_{t=0}^\infty r_t]$ ; $g = \mathbb{E}[\sum_{t=0}^\infty\psi_t\nabla_\theta log\pi_\theta(a_t \vert s_t)]$\\[5pt]
\begin{Large}{$\psi_t$ 可能方法為下列:}\end{Large}
\begin{itemize}
\item $\sum_{t=0}^\infty r$：決策軌跡的獎勵總和。
\item $\sum_{t^{'}=t}^\infty r^{'}$: 根據動作(action)的獎勵(reward) $a_t$。\\
標準表示式：$\sum_{t^{'}=t}^\infty r_t^{'}-b(s_t)$
\item $Q^\pi(s_t,a_t)$：state-action value function。
\item $A^\pi(s_t,a_t)$：Advantage Function。
\item $r_t+V^\pi(s_t+1)-V^\pi(s_t)$：TD residual。
\end{itemize}
公式使用定義：\\[5pt]
$V^\pi(s_t) = \mathbb{E}_{s_{t}+1:\infty,a_{t}:\infty}[\sum_{l=0}^\infty r_t+l]$\\[5pt]
$Q^\pi(s_t,a_t) = \mathbb{E}_{s_{t}+1:\infty_,a_{t}+1:\infty}[\sum_{l=0}^\infty r_t+l]$\\[5pt]
$A^\pi(s_t,a_t) = Q^\pi(s_t,a_t)-V^\pi(s_t)(Advantage Function)$\\[5pt]
\section{類神經網路中強化學習的應用}
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.74]{network}
\caption{實際兩層神經網路}
\end{center}
\end{figure}

 我們將定義一個可以執行玩家(agent)的類神經網路，該網路將獲取遊戲狀態並決定我們應該做什麼(向上移動或向下移動我們使用一個2層神經網路，該網路獲取原始圖像像素(100,800個數字(210*160*3))，並生成一個表示上升概率的數字。 使用隨機策略是標準做法，這意味著我們只會產生向上移動的可能性，每次迭代時，我們都會從該分布中採樣(即扔一枚有偏見的硬幣)已獲得實際移動。\\

\subsection{監督式學習}
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.74]{supervising_learning}
\caption{監督式學習}
\end{center}
\end{figure}

 在我們深入探討 Score Function 解決方案之前，需要簡短介紹有關監督學習的知識，因為正如看到的，與我們架構類似，在普通的監督學習中，我們會將圖像傳送到網路，並獲得一概率值，例如對於兩個類別的上和下。 這裡顯示的是向上和向下對數概率(-1.2,-0.36)，而不是原始機率(在這個情況下，是30$\%$和70$\%$)，因為我們總是優化正確標籤的對數概率(這使我們的演算法更好，並等效於優化原始概率，因為對數是單調的)，而在監督學習中，我們將可以獲取標籤，例如:
我們可能被告知現在正確的做法是向上運動(標籤0)，在執行過程中，我們將以上的對數機率輸入1.0的梯度，然後運行反向傳播來計算梯度向量$Wlogp(y=UP|x)$這個梯度將告訴我們應如何更改百萬個參數中的每個參數，使網路預測往上的可能性更高，例如:網路中的百萬個參數之一可能具有-2.1的梯度，這意味著如果我們將該參數增加一個小的正值(例如0.001)，則往上的對數機率將因2.1*0.001而降低(由於負號而減少)，如果我們隨後更新了參數，當之後遇到非常相似的圖像時(也就是環境狀況)，我們的網路現在更有可能預測往上。\\

\subsection{對數導數技巧}
 機器學習涉及操縱機率。這個機率通常包含歸一化概率或對數概率。能加強解決現代機器學習問題的關鍵點，是能夠巧妙的在這兩種型式間交替使用，而對數導數技巧就能夠幫助我們做到這點，也就是運用對數導數的性質。\\
 
\subsection{為什麼選擇score function 算法}
 多篇論文已經廣泛使用了ATARI遊戲並結合了DQN(它是一種在強化學習算法裡，知名度較高的)，事實證明，Q-Learning並不是一個很好的算法，實際上大多數人比較喜歡使用Policy Gradients，包括原始DQN論文的作者，他們在調優後顯示Policy Gradients比Q-Learning運作得更好，首選PG是因為它是端到端的：有一個明確的政策和一種有原則的方法可以直接優化預期的回報。但是礙於時間考量，而選擇了類似PG的算法，也就是score function gradient estimator(取用Andrej Karpathy)，從像素開始，通過類神經網路加上強化學習結合ATARI遊戲（Pong），在整個過程使用numpy運算，作為訓練工具。\\ 
 
\subsection{Score Functions}
 對數導數技巧的應用規則是基於參數$\theta$梯度的對數函數$p(x:\theta)$，如下:\\
$$\nabla_\theta logp(x:\theta)=\frac{\nabla_\theta p(x:\theta)}{p(x:\theta)}$$\\
$p(x:\theta)$是likelihood ; function參數$\theta$的函數，它提供隨機變量x的概率。在此特例中，$\nabla_\theta logp(x:\theta)$被稱為Score Function，而上述方程式右邊為score ratio(得分比)。\\
score function具有許多有用的屬性:\\

\begin{itemize}
\item 最大概似估計的中央計算。最大概似是機器學習中使用的學習原理之一，用於廣義線性回歸、深度學習、kernel machines、降維和張量分解等，而score出現在這些所有問題中。
\end{itemize}
\begin{itemize}
\item  score的期望值為零。對數導數技巧的第一個用途就是證明這一點。\\
$$\mathbb{E}_{p(x; \theta)}[\nabla_\theta \log p(\mathbf{x}; \theta)] =\mathbb{E}_{p(x; \theta)}\left[\frac{\nabla_\theta p(\mathbf {x}; \theta)}{p(\mathbf{x}; \theta)} \right]$$
$$= \int p(\mathbf {x}; \theta) \frac{\nabla_\theta p(\mathbf {x}; \theta)}{p(\mathbf{x}; \theta)} dx= \nabla_\theta \int p(\mathbf{x}; \theta) dx=\nabla_\theta 1 = 0$$\\
\qquad 在第一行中，我們應用了對數導數技巧，在第二行中，我們交換了差異化和積分的順序，這種特性是我們尋求概率靈活性的類型:  它允許我們從期望值為零的分數中減去任何一項，且此修改不會影響預期得分(控制變量)。
\end{itemize}
\begin{itemize}
\item 得分的方差是Fisher信息，用於確定Cramer-Rao下限。\\
$$\mathbb{V}[\nabla_\theta \log p(\mathbf{x}; \theta)] = \mathcal{I}(\theta) =\mathbb{E}_{p(x; \theta)}[\nabla_\theta \log p(\mathbf{x}; \theta)\nabla_\theta \log p(\mathbf{x}; \theta)^\top]$$\\
我們現在可以從對數概率的梯度躍升為概率的梯度，然後返回，但是真正要解決的其實是計算困難的期望梯度，所以我們可以利用新發現的功能:score function為此問題開發另一個聰明的估計器。
\end{itemize}
\subsection{Score Function Estimators}
我們的問題是計算函數f的期望值的梯度：\\
$$\nabla_\theta \mathbb{E}_{p(z;\theta)}[f(z)] =\nabla_\theta \int p(z; \theta)f(z) dz$$
 這是機器學習中的一項常態性任務，在變數推理中進行後驗計算，在強化學習中進行價值函數和策略學習，在計算金融中進行衍生產品定價以及在運籌學中進行庫存控制等。該梯度很難計算，因為積分通常是未知的，我們計算梯度所依據的參數θ的分佈為p（z;θ)，此外，當函數f不可微時，我們可能想計算該梯度，使用對數導數技巧和得分函數的屬性，我們可以更方便地計算此梯度：\\
$$\nabla_\theta \mathbb{E}_{p(z;\theta)}[f(z)] = \mathbb{E}_{p(z;\theta)}[f(z)\nabla_\theta \log p(z;\theta)]$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.5]{gradient_change}
\caption{梯度變化}
\end{center}
\end{figure}
 讓我們導出該表達式，並探討它對我們的優化問題的影響。\\
 為此，我們將使用另一種普遍存在的技巧，一種概率恆等的技巧，在該技巧中，我們將表達式乘以1，該表達式由概率密度除以自身而形成。將特性技巧與對數導數技巧相結合，我們獲得了梯度的得分函數估計量:\\
$$\nabla_\theta \mathbb{E}_{p(z;\theta)}[f(z)]=\int\nabla_\theta p(z;\theta)f(z) dz$$
$$= \int \frac{p(z;\theta)}{p(z;\theta)}\nabla_\theta p(z;\theta)f(z) dz$$
$$=\int p(z;\theta)\nabla_\theta \log p(z;\theta)f(z) dz = \mathbb{E}_{p(z;\theta)}[f(z)\nabla_\theta \log p(z;\theta)]$$
$$=\int p(z;\theta)\nabla_\theta \log p(z;\theta)f(z) dz = \mathbb{E}_{p(z;\theta)}[f(z)\nabla_\theta \log p(z;\theta)]$$
$$\approx \frac{1}{S} \sum_{s=1}^{S}f(z^{(s)})\nabla_\theta \log p(z^{(s)};\theta) \quad z^{(s)}\sim p(z)$$\\
在這四行中發生了很多事情。在第一行中，我們交換了導數和積分。在第二行中，我們應用了概率身份技巧，這使我們能夠形成得分比， 然後使用對數導數技巧，用第三行中對數概率的梯度替換該比率。這在第四行給出了我們所需的隨機估計量，這是由蒙特卡洛計算的，方法是首先從p（z）提取樣本，然後計算加權梯度項。\\

更簡單的描述，我們有一些分佈 $p(x;\theta)$（我們使用了速記$ p \left( x \right)$ 來減少混亂），我們可以從中採樣（例如，這可能是高斯）。對於每個樣本，我們還可以評估score function f(x)，該函數將樣本作為樣本並給出標量值。該方程式告訴我們，如果我們希望其樣本達到較高的分數（由f判斷），應該如何改變分佈（通過其參數θ)，特別是，它看起來像：畫出一些樣本x，評估其分數f（x），並且對於每個x也評估第二項 $\nabla_\theta logp(x;θ)$，第二項是一個漸變向量為我們提供了參數空間中的方向，使分配給x的概率增加。換句話說，如果我們要在的方向上微移θ，$\nabla_\theta logp(x;θ)$，分配給x的新概率略有增加，朝這個方向移動，並將標量值加到上面$f(x)$。根據p(x)上的幾個樣本進行更新，則概率密度將朝著較高分數的方向移動，從而使得分較高的樣本更有可能出現。\\
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.4]{figure}
\caption{Score Function可視圖}
\end{center}
\end{figure}

 Score function gradient estimator的可視化，左：高斯分佈及其中的一些樣本（藍點)，在每個藍點上，我們還繪製了相對於高斯平均參數的對數概率的梯度，箭頭指示應微調分佈平均值以增加該樣本概率的方向。中間：某些得分函數的疊加，在某些小區域中除了+1之外，其他所有地方都給出-1，箭頭採用顏色區別，更新運用乘法運算，我們將平均所有綠色箭頭和紅色箭頭。右：更新參數後，綠色箭頭和反向紅色箭頭將向左移至底部。現在，根據需要，該分佈中的樣本將具有更高的預期分數。\\
\fi
\newpage
